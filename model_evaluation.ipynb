{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b74af90-0999-485c-b4a2-3fbb33aefc1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a514701-4bdb-4571-8d4c-e83d6e109e0d",
   "metadata": {},
   "source": [
    "# Exercises I"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a202c34a-05ea-4f5b-8480-6c10db28fadf",
   "metadata": {},
   "source": [
    "### 1. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc6373-60f2-4bc7-ace4-40b52f48a924",
   "metadata": {},
   "source": [
    "#### 1. 1. In the context of this problem, what is a false positive?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72220aab-af22-4723-a5f7-7488a32521c3",
   "metadata": {},
   "source": [
    "There are 13 False Positives, where we predicted dog but the result was cat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458589ef-c426-40cb-ba40-37271536bba5",
   "metadata": {},
   "source": [
    "#### 1. 2. In the context of this problem, what is a false negative?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9981334e-80db-4c31-8c8a-592e60aafeb2",
   "metadata": {},
   "source": [
    "There are 7 False Negatives, where we predicted cat but the result was dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d79aa07-b80f-4c44-bad0-e5984d713a3a",
   "metadata": {},
   "source": [
    "#### 1. 3. How would you describe this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4910a92-d5e7-4141-9c20-3594b53757cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's overall accuracy of predicting dog correctly is 80.39%\n"
     ]
    }
   ],
   "source": [
    "acc_dog = round((46+36)/(46+36+7+13)*100 ,2)\n",
    "print(f\"The model's overall accuracy of predicting dog correctly is {acc_dog}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5ed47f-448c-4818-bcbc-0db17534d0a3",
   "metadata": {},
   "source": [
    "I'd say it's a pretty good model, accuracy is pretty viable but I'd love to see much more samples to determine a better overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce3be-36b4-4ac0-9bd1-feba0bae1b61",
   "metadata": {},
   "source": [
    "### 2. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant. \n",
    "### Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6adcb00-cf7e-40c6-aced-4c4e198a861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual     model1     model2     model3\n",
       "0    No Defect  No Defect     Defect  No Defect\n",
       "1    No Defect  No Defect     Defect     Defect\n",
       "2    No Defect  No Defect     Defect  No Defect\n",
       "3    No Defect     Defect     Defect     Defect\n",
       "4    No Defect  No Defect     Defect  No Defect\n",
       "..         ...        ...        ...        ...\n",
       "195  No Defect  No Defect     Defect     Defect\n",
       "196     Defect     Defect  No Defect  No Defect\n",
       "197  No Defect  No Defect  No Defect  No Defect\n",
       "198  No Defect  No Defect     Defect     Defect\n",
       "199  No Defect  No Defect  No Defect     Defect\n",
       "\n",
       "[200 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C3 = pd.read_csv('/Users/jay/codeup-data-science/Classification/classification-exercises/c3.csv')\n",
    "C3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d3a4f-2d0a-45c6-b8a5-6fe5f0dd737b",
   "metadata": {},
   "source": [
    "#### 2. 1. Use the predictions dataset and pandas to help answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bf06f1-4709-4d67-89d6-3038495bb79a",
   "metadata": {},
   "source": [
    "##### 2. 1. 1. An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557356d7-0ec6-4109-9fdb-fec09ee4bc91",
   "metadata": {},
   "source": [
    "The recall evaluation will give us the best representation of how well the models captured the defects, model 3 had the highest percentage at 81.25%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "67df8aa0-1df1-4e01-89ee-c403ac1c3389",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "C3['baseline_prediction'] = 'Defect'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c828462b-7924-48d2-ac6a-f065f2ea93c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual\n",
       "No Defect    184\n",
       "Defect        16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C3['actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a90cf7e4-98f2-476a-a1be-d473ed5fee2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model1                      \n",
       "Defect          8          2\n",
       "No Defect       8        182"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = pd.crosstab(C3['actual'], C3['model1']).T\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2696cf20-d2e2-4dd3-8bc2-9e9047148605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(8)/(8+8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f741af53-2978-48ec-9a90-af1eb50378e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model2                      \n",
       "Defect          9         81\n",
       "No Defect       7        103"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = pd.crosstab(C3['actual'], C3['model2']).T\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98d14b99-042e-4eb6-abb0-cc6578115f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(9)/(9+7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16f24252-8a98-4dfd-90d4-6fa339bd34d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>13</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model3                      \n",
       "Defect         13         86\n",
       "No Defect       3         98"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = pd.crosstab(C3['actual'], C3['model3']).T\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "271eaa46-b71b-454e-8b6e-68d39ed56aaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(13)/(3+13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e6f83ac5-7c11-4dd7-b7d9-c02beebc695e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model recall 1: 50.00%\n",
      "  model recall 2: 56.25%\n",
      "  model recall 3: 81.25%\n"
     ]
    }
   ],
   "source": [
    "subset = C3[C3['actual'] == 'Defect']\n",
    "model_recall1 = (subset['model1'] == subset['actual']).mean()\n",
    "model_recall2 = (subset['model2'] == subset['actual']).mean()\n",
    "model_recall3 = (subset['model3'] == subset['actual']).mean()\n",
    "print(f'  model recall 1: {model_recall1:.2%}')\n",
    "print(f'  model recall 2: {model_recall2:.2%}')\n",
    "print(f'  model recall 3: {model_recall3:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6411c7-1e07-44a1-ab1a-c20fcfe5e636",
   "metadata": {},
   "source": [
    "##### 2. 1. 2. Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you the really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. Which evaluation metric would be appropriate here? Which model would be the best fit for this use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe59e5-30d9-41da-ba7a-4b1e5a42010e",
   "metadata": {},
   "source": [
    "A precision evaluation was needed since we are trying to find all Defects whether actual or False Positive. Model 3 had the best model precision at 97.03% to the baseline 92%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "966fb695-a89e-4c99-a835-932bcf1f59d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model1                      \n",
       "Defect          8          2\n",
       "No Defect       8        182"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea56e55b-91b4-42a5-aed4-1101acc7a8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision: 95.79%\n",
      "baseline precision: 92.00%\n"
     ]
    }
   ],
   "source": [
    "subset = C3[C3['model1'] == 'Defect']\n",
    "model_precision = (subset.model1 == subset.actual).mean()\n",
    "\n",
    "subset = C3[C3.baseline_prediction == 'Defect']\n",
    "baseline_precision = (subset.baseline_prediction == subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea70ebd0-d29a-4b90-b2dd-17f34b56e570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>9</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>7</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model2                      \n",
       "Defect          9         81\n",
       "No Defect       7        103"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9b58d8b-8884-4e40-8bde-72c6d188819c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision: 93.64%\n",
      "baseline precision: 92.00%\n"
     ]
    }
   ],
   "source": [
    "subset = C3[C3['model2'] == 'No Defect']\n",
    "model_precision = (subset.model2 == subset.actual).mean()\n",
    "\n",
    "subset = C3[C3.baseline_prediction == 'No Defect']\n",
    "baseline_precision = (subset.baseline_prediction == subset.actual).mean()\n",
    "\n",
    "print(f'model precision: {model_precision:.2%}')\n",
    "print(f'baseline precision: {baseline_precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce439840-8116-4c70-a825-d10659f3d77d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>13</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>3</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual     Defect  No Defect\n",
       "model3                      \n",
       "Defect         13         86\n",
       "No Defect       3         98"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "73c9330d-f538-4dda-90db-bc457948cf52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model precision 1: 50.00%\n",
      "model precision 2: 56.25%\n",
      "model precision 3: 81.25%\n"
     ]
    }
   ],
   "source": [
    "subset1 = C3[C3['model1'] == 'Defect']\n",
    "model_precision1 = (subset.model1 == subset.actual).mean()\n",
    "subset2 = C3[C3['model2'] == 'Defect']\n",
    "model_precision2 = (subset.model2 == subset.actual).mean()\n",
    "subset3 = C3[C3['model3'] == 'Defect']\n",
    "model_precision3 = (subset.model3 == subset.actual).mean()\n",
    "\n",
    "print(f'model precision 1: {model_precision1:.2%}')\n",
    "print(f'model precision 2: {model_precision2:.2%}')\n",
    "print(f'model precision 3: {model_precision3:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f612ea-af8e-4648-846c-12cae107e4be",
   "metadata": {},
   "source": [
    "### 3. You are working as a data scientist for Gives You Paws â„¢, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee). At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. First an automated algorithm tags pictures as either a cat or a dog (Phase I). Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3e1f7d-9979-4315-8c68-e7464de0bfaa",
   "metadata": {},
   "source": [
    "#### 3. 1. Several models have already been developed with the data, and you can find their results here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d48d77b7-b33c-43cf-984d-73c0acad065c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual model1 model2 model3 model4\n",
       "0       cat    cat    dog    cat    dog\n",
       "1       dog    dog    cat    cat    dog\n",
       "2       dog    cat    cat    cat    dog\n",
       "3       dog    dog    dog    cat    dog\n",
       "4       cat    cat    cat    dog    dog\n",
       "...     ...    ...    ...    ...    ...\n",
       "4995    dog    dog    dog    dog    dog\n",
       "4996    dog    dog    cat    cat    dog\n",
       "4997    dog    cat    cat    dog    dog\n",
       "4998    cat    cat    cat    cat    dog\n",
       "4999    dog    dog    dog    dog    dog\n",
       "\n",
       "[5000 rows x 5 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws = pd.read_csv('/Users/jay/codeup-data-science/Classification/classification-exercises/gives_you_paws.csv')\n",
    "paws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5fcf53-6ed8-4620-85c3-fb30d800c479",
   "metadata": {},
   "source": [
    "#### 3. 2. Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c19a850-d7de-401f-9d08-2678593ace1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual\n",
       "dog    3254\n",
       "cat    1746\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paws['actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce772468-5e9a-4f3e-8c05-9cd5767b0213",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paws['baseline_prediction'] = 'dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "57e5ce66-5618-47b7-84db-b5d4198308cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6508"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_accuracy = (paws.baseline_prediction == paws.actual).mean()\n",
    "baseline_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54c95e5-b0a4-4b72-9450-11dd691f4aca",
   "metadata": {},
   "source": [
    "##### 3. 2. 1. In terms of accuracy, how do the various models compare to the baseline model? Are any of the models better than the baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec55d709-8fff-4be3-890b-f508b76d8cf6",
   "metadata": {},
   "source": [
    "In terms of accuracy, Model 1 performs the best at 80.74% to the baseline accuracy of 65.08%. Model 4 also beat baseline at 74% to baseline 65%. Model 2 and 3 perform worse than baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d5c93279-38ff-49d6-8b35-c411029b4250",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1423</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>323</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model1            \n",
       "cat     1423   640\n",
       "dog      323  2614"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = pd.crosstab(paws['actual'], paws['model1']).T\n",
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e59b794-be8b-440c-9e67-5dd38f94db26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8074"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1423+2614)/(1423+2614+640+323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ef7a0e6-4e1d-40f1-9c31-88b445b864b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1555</td>\n",
       "      <td>1657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>191</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model2            \n",
       "cat     1555  1657\n",
       "dog      191  1597"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = pd.crosstab(paws['actual'], paws['model2']).T\n",
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1e11695-e6a6-49d6-8db1-40250af99800",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6304"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1555+1597)/(1555+1597+1657+191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba0baa92-a69d-49e4-9f1d-d8054fc359ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>893</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>853</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual  cat   dog\n",
       "model3           \n",
       "cat     893  1599\n",
       "dog     853  1655"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = pd.crosstab(paws['actual'], paws['model3']).T\n",
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f92cc26c-d88e-4e53-9de4-050dd13501d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5096"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(893+1655)/(893+1599+853+1655)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe1d5069-1f31-44f4-85a2-ed6fff170ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline accuracy: 65.08%\n",
      " model 1 accuracy: 80.74%\n",
      " model 2 accuracy: 63.04%\n",
      " model 3 accuracy: 50.96%\n",
      " model 4 accuracy: 74.26%\n"
     ]
    }
   ],
   "source": [
    "baseline_accuracy = (paws.baseline_prediction == paws.actual).mean()\n",
    "model_accuracy1 = (paws.model1 == paws.actual).mean()\n",
    "model_accuracy2 = (paws.model2 == paws.actual).mean()\n",
    "model_accuracy3 = (paws.model3 == paws.actual).mean()\n",
    "model_accuracy4 = (paws.model4 == paws.actual).mean()\n",
    "\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')\n",
    "print(f' model 1 accuracy: {model_accuracy1:.2%}')\n",
    "print(f' model 2 accuracy: {model_accuracy2:.2%}')\n",
    "print(f' model 3 accuracy: {model_accuracy3:.2%}')\n",
    "print(f' model 4 accuracy: {model_accuracy4:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88932fc4-73c2-492d-befe-b7fcf2f6411c",
   "metadata": {},
   "source": [
    "##### 3. 2. 2. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480aa123-c605-4059-ac45-1652fd12a3cd",
   "metadata": {},
   "source": [
    "Model 4 has the highest precision accuracy for predicting dogs at almost 96%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e715f19f-22b1-474a-af6a-669815a89f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1423</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>323</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model1            \n",
       "cat     1423   640\n",
       "dog      323  2614"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d0e882b5-d920-485d-867d-d2dd4fe6ae87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8900238338440586"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(2614)/(2614+323)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf9b22e5-c7e6-41d0-88a3-3d2807da40c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1555</td>\n",
       "      <td>1657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>191</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model2            \n",
       "cat     1555  1657\n",
       "dog      191  1597"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2293bbb3-077b-4da6-93c5-d375da5e395f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8931767337807607"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1597)/(1597+191)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dea4d29-490c-4329-8cb8-f5785b044feb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>893</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>853</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual  cat   dog\n",
       "model3           \n",
       "cat     893  1599\n",
       "dog     853  1655"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "90c3e2a3-dabb-4605-a636-5c7cecaeccfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6598883572567783"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1655)/(1655+853)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "4d591aca-1eea-408e-b0af-3888e88e3086",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model 1 recall: 80.33%\n",
      "   model 2 recall: 49.08%\n",
      "   model 3 recall: 50.86%\n",
      "   model 4 recall: 95.57%\n",
      "baseline accuracy: 65.08%\n"
     ]
    }
   ],
   "source": [
    "paws['baseline_prediction'] = 'dog'\n",
    "subset = paws[paws['actual'] == 'dog']\n",
    "baseline_accuracy = (paws.baseline_prediction == paws.actual).mean()\n",
    "model_recall1 = (subset['model1'] == subset['actual']).mean()\n",
    "model_recall2 = (subset['model2'] == subset['actual']).mean()\n",
    "model_recall3 = (subset['model3'] == subset['actual']).mean()\n",
    "model_recall4 = (subset['model4'] == subset['actual']).mean()\n",
    "print(f'   model 1 recall: {model_recall1:.2%}')\n",
    "print(f'   model 2 recall: {model_recall2:.2%}')\n",
    "print(f'   model 3 recall: {model_recall3:.2%}')\n",
    "print(f'   model 4 recall: {model_recall4:.2%}')\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0e9375-256d-40b1-b68e-b8fed9f2b9be",
   "metadata": {},
   "source": [
    "##### 3. 2. 3. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recommend?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0786deee-f392-4cbe-9cac-2aa8c49942a6",
   "metadata": {},
   "source": [
    "Model 2 has the best recall percentage at 89.06%. \n",
    "\n",
    "Model 1 is close behind it at 81.50%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abdd3655-98d2-47a2-8b36-aa59f88024ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1423</td>\n",
       "      <td>640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>323</td>\n",
       "      <td>2614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model1            \n",
       "cat     1423   640\n",
       "dog      323  2614"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44b53cf1-f851-44d7-9d17-133d6a1c4713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6897721764420747"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1423)/(1423+640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d7b15965-ad7e-4bdd-aa50-ed96b0e3da3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model2</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1555</td>\n",
       "      <td>1657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>191</td>\n",
       "      <td>1597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual   cat   dog\n",
       "model2            \n",
       "cat     1555  1657\n",
       "dog      191  1597"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a3c3db31-e6d0-45dc-aee5-b4b2dc6242e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4841220423412204"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1555)/(1555+1657)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5fb3127-530d-4e6c-82c3-c99e28fc9f39",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>actual</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model3</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>893</td>\n",
       "      <td>1599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>853</td>\n",
       "      <td>1655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "actual  cat   dog\n",
       "model3           \n",
       "cat     893  1599\n",
       "dog     853  1655"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2de6dc5e-c048-471b-9f46-79dc8d9e0978",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.358346709470305"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(893)/(893+1599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aa98203b-4c4f-403f-8b4e-56927b74e33c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   model 1 recall: 81.50%\n",
      "   model 2 recall: 89.06%\n",
      "   model 3 recall: 51.15%\n",
      "   model 4 recall: 34.54%\n",
      "baseline accuracy: 34.92%\n"
     ]
    }
   ],
   "source": [
    "paws['baseline_prediction'] = 'cat'\n",
    "subset = paws[paws['actual'] == 'cat']\n",
    "baseline_accuracy = (paws.baseline_prediction == paws.actual).mean()\n",
    "model_recall1 = (subset['model1'] == subset['actual']).mean()\n",
    "model_recall2 = (subset['model2'] == subset['actual']).mean()\n",
    "model_recall3 = (subset['model3'] == subset['actual']).mean()\n",
    "model_recall4 = (subset['model4'] == subset['actual']).mean()\n",
    "print(f'   model 1 recall: {model_recall1:.2%}')\n",
    "print(f'   model 2 recall: {model_recall2:.2%}')\n",
    "print(f'   model 3 recall: {model_recall3:.2%}')\n",
    "print(f'   model 4 recall: {model_recall4:.2%}')\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2701fa21-5f18-495b-87e8-5876a6df7857",
   "metadata": {},
   "source": [
    "### 4. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe840c-b3b6-44c7-9e8e-ea0fe6ebfb5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4. 1. sklearn.metrics.accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e8627638-4fe1-424a-bcde-5b53d89feff8",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Accuracy classification score.\n",
       "\n",
       "In multilabel classification, this function computes subset accuracy:\n",
       "the set of labels predicted for a sample must *exactly* match the\n",
       "corresponding set of labels in y_true.\n",
       "\n",
       "Read more in the :ref:`User Guide <accuracy_score>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) labels.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Predicted labels, as returned by a classifier.\n",
       "\n",
       "normalize : bool, default=True\n",
       "    If ``False``, return the number of correctly classified samples.\n",
       "    Otherwise, return the fraction of correctly classified samples.\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "score : float\n",
       "    If ``normalize == True``, return the fraction of correctly\n",
       "    classified samples (float), else returns the number of correctly\n",
       "    classified samples (int).\n",
       "\n",
       "    The best performance is 1 with ``normalize == True`` and the number\n",
       "    of samples with ``normalize == False``.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "balanced_accuracy_score : Compute the balanced accuracy to deal with\n",
       "    imbalanced datasets.\n",
       "jaccard_score : Compute the Jaccard similarity coefficient score.\n",
       "hamming_loss : Compute the average Hamming loss or Hamming distance between\n",
       "    two sets of samples.\n",
       "zero_one_loss : Compute the Zero-one classification loss. By default, the\n",
       "    function will return the percentage of imperfectly predicted subsets.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "In binary classification, this function is equal to the `jaccard_score`\n",
       "function.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import accuracy_score\n",
       ">>> y_pred = [0, 2, 1, 3]\n",
       ">>> y_true = [0, 1, 2, 3]\n",
       ">>> accuracy_score(y_true, y_pred)\n",
       "0.5\n",
       ">>> accuracy_score(y_true, y_pred, normalize=False)\n",
       "2\n",
       "\n",
       "In the multilabel case with binary label indicators:\n",
       "\n",
       ">>> import numpy as np\n",
       ">>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
       "0.5\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "d3690673-bbdc-41af-8f36-a101c1911081",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for Model 1 is 80.74%\n"
     ]
    }
   ],
   "source": [
    "acc_mod_1 = accuracy_score(paws['model1'],paws['actual'])\n",
    "print(f'The accuracy score for Model 1 is {acc_mod_1:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b2f337a9-3eb5-4a62-84d0-1d9f8f58c0e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for Model 1 is 63.04%\n"
     ]
    }
   ],
   "source": [
    "acc_mod_2 = accuracy_score(paws['model2'],paws['actual'])\n",
    "print(f'The accuracy score for Model 1 is {acc_mod_2:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9535f768-b0ec-4bda-bcd8-e366660e8a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for Model 1 is 50.96%\n"
     ]
    }
   ],
   "source": [
    "acc_mod_3 = accuracy_score(paws['model3'],paws['actual'])\n",
    "print(f'The accuracy score for Model 1 is {acc_mod_3:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d727b712-a4b5-4da9-a060-180c847e8796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score for Model 1 is 74.26%\n"
     ]
    }
   ],
   "source": [
    "acc_mod_4 = accuracy_score(paws['model4'],paws['actual'])\n",
    "print(f'The accuracy score for Model 1 is {acc_mod_4:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6786b357-195c-49f6-a2eb-3d95ebe2f956",
   "metadata": {},
   "source": [
    "#### 4. 2. sklearn.metrics.precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1e814451-d96b-4216-936d-689a35d0b8b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute the precision.\n",
       "\n",
       "The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
       "true positives and ``fp`` the number of false positives. The precision is\n",
       "intuitively the ability of the classifier not to label as positive a sample\n",
       "that is negative.\n",
       "\n",
       "The best value is 1 and the worst value is 0.\n",
       "\n",
       "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Estimated targets as returned by a classifier.\n",
       "\n",
       "labels : array-like, default=None\n",
       "    The set of labels to include when ``average != 'binary'``, and their\n",
       "    order if ``average is None``. Labels present in the data can be\n",
       "    excluded, for example to calculate a multiclass average ignoring a\n",
       "    majority negative class, while labels not present in the data will\n",
       "    result in 0 components in a macro average. For multilabel targets,\n",
       "    labels are column indices. By default, all labels in ``y_true`` and\n",
       "    ``y_pred`` are used in sorted order.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       Parameter `labels` improved for multiclass problem.\n",
       "\n",
       "pos_label : str or int, default=1\n",
       "    The class to report if ``average='binary'`` and the data is binary.\n",
       "    If the data are multiclass or multilabel, this will be ignored;\n",
       "    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
       "    scores for that label only.\n",
       "\n",
       "average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
       "    This parameter is required for multiclass/multilabel targets.\n",
       "    If ``None``, the scores for each class are returned. Otherwise, this\n",
       "    determines the type of averaging performed on the data:\n",
       "\n",
       "    ``'binary'``:\n",
       "        Only report results for the class specified by ``pos_label``.\n",
       "        This is applicable only if targets (``y_{true,pred}``) are binary.\n",
       "    ``'micro'``:\n",
       "        Calculate metrics globally by counting the total true positives,\n",
       "        false negatives and false positives.\n",
       "    ``'macro'``:\n",
       "        Calculate metrics for each label, and find their unweighted\n",
       "        mean.  This does not take label imbalance into account.\n",
       "    ``'weighted'``:\n",
       "        Calculate metrics for each label, and find their average weighted\n",
       "        by support (the number of true instances for each label). This\n",
       "        alters 'macro' to account for label imbalance; it can result in an\n",
       "        F-score that is not between precision and recall.\n",
       "    ``'samples'``:\n",
       "        Calculate metrics for each instance, and find their average (only\n",
       "        meaningful for multilabel classification where this differs from\n",
       "        :func:`accuracy_score`).\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
       "    Sets the value to return when there is a zero division. If set to\n",
       "    \"warn\", this acts as 0, but warnings are also raised.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "precision : float (if average is not None) or array of float of shape                 (n_unique_labels,)\n",
       "    Precision of the positive class in binary classification or weighted\n",
       "    average of the precision of each class for the multiclass task.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
       "    support for each class.\n",
       "recall_score :  Compute the ratio ``tp / (tp + fn)`` where ``tp`` is the\n",
       "    number of true positives and ``fn`` the number of false negatives.\n",
       "PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
       "    an estimator and some data.\n",
       "PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
       "    binary class predictions.\n",
       "multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
       "    sample.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When ``true positive + false positive == 0``, precision returns 0 and\n",
       "raises ``UndefinedMetricWarning``. This behavior can be\n",
       "modified with ``zero_division``.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import precision_score\n",
       ">>> y_true = [0, 1, 2, 0, 1, 2]\n",
       ">>> y_pred = [0, 2, 1, 0, 0, 1]\n",
       ">>> precision_score(y_true, y_pred, average='macro')\n",
       "0.22...\n",
       ">>> precision_score(y_true, y_pred, average='micro')\n",
       "0.33...\n",
       ">>> precision_score(y_true, y_pred, average='weighted')\n",
       "0.22...\n",
       ">>> precision_score(y_true, y_pred, average=None)\n",
       "array([0.66..., 0.        , 0.        ])\n",
       ">>> y_pred = [0, 0, 0, 0, 0, 0]\n",
       ">>> precision_score(y_true, y_pred, average=None)\n",
       "array([0.33..., 0.        , 0.        ])\n",
       ">>> precision_score(y_true, y_pred, average=None, zero_division=1)\n",
       "array([0.33..., 1.        , 1.        ])\n",
       ">>> # multilabel classification\n",
       ">>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
       ">>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
       ">>> precision_score(y_true, y_pred, average=None)\n",
       "array([0.5, 1. , 1. ])\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precision_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2dab5f00-fbe2-43c3-bc66-241899d722a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for Model 1 is 89.00%\n"
     ]
    }
   ],
   "source": [
    "prec_mod_1 = precision_score(paws['actual'], paws['model1'], pos_label ='dog')\n",
    "print(f'The precision score for Model 1 is {prec_mod_1:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c6809e5f-1664-4f29-a0c3-0e0bbe594c78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for Model 1 is 89.32%\n"
     ]
    }
   ],
   "source": [
    "prec_mod_2 = precision_score(paws['actual'], paws['model2'], pos_label ='dog')\n",
    "print(f'The precision score for Model 1 is {prec_mod_2:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "08635c06-b9ab-4bf3-99dc-ec0335d860ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for Model 1 is 65.99%\n"
     ]
    }
   ],
   "source": [
    "prec_mod_3 = precision_score(paws['actual'], paws['model3'], pos_label ='dog')\n",
    "print(f'The precision score for Model 1 is {prec_mod_3:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "dd54cb59-7707-407b-bfb7-cc1c450115d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision score for Model 1 is 73.12%\n"
     ]
    }
   ],
   "source": [
    "prec_mod_4 = precision_score(paws['actual'], paws['model4'], pos_label ='dog')\n",
    "print(f'The precision score for Model 1 is {prec_mod_4:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8318d653-2f63-4481-acfb-cbfa25cbdfac",
   "metadata": {},
   "source": [
    "#### 4. 3. sklearn.metrics.recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6df5cae7-9f84-4cdd-a3ec-68305640cf63",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute the recall.\n",
       "\n",
       "The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
       "true positives and ``fn`` the number of false negatives. The recall is\n",
       "intuitively the ability of the classifier to find all the positive samples.\n",
       "\n",
       "The best value is 1 and the worst value is 0.\n",
       "\n",
       "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Estimated targets as returned by a classifier.\n",
       "\n",
       "labels : array-like, default=None\n",
       "    The set of labels to include when ``average != 'binary'``, and their\n",
       "    order if ``average is None``. Labels present in the data can be\n",
       "    excluded, for example to calculate a multiclass average ignoring a\n",
       "    majority negative class, while labels not present in the data will\n",
       "    result in 0 components in a macro average. For multilabel targets,\n",
       "    labels are column indices. By default, all labels in ``y_true`` and\n",
       "    ``y_pred`` are used in sorted order.\n",
       "\n",
       "    .. versionchanged:: 0.17\n",
       "       Parameter `labels` improved for multiclass problem.\n",
       "\n",
       "pos_label : str or int, default=1\n",
       "    The class to report if ``average='binary'`` and the data is binary.\n",
       "    If the data are multiclass or multilabel, this will be ignored;\n",
       "    setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
       "    scores for that label only.\n",
       "\n",
       "average : {'micro', 'macro', 'samples', 'weighted', 'binary'} or None,             default='binary'\n",
       "    This parameter is required for multiclass/multilabel targets.\n",
       "    If ``None``, the scores for each class are returned. Otherwise, this\n",
       "    determines the type of averaging performed on the data:\n",
       "\n",
       "    ``'binary'``:\n",
       "        Only report results for the class specified by ``pos_label``.\n",
       "        This is applicable only if targets (``y_{true,pred}``) are binary.\n",
       "    ``'micro'``:\n",
       "        Calculate metrics globally by counting the total true positives,\n",
       "        false negatives and false positives.\n",
       "    ``'macro'``:\n",
       "        Calculate metrics for each label, and find their unweighted\n",
       "        mean.  This does not take label imbalance into account.\n",
       "    ``'weighted'``:\n",
       "        Calculate metrics for each label, and find their average weighted\n",
       "        by support (the number of true instances for each label). This\n",
       "        alters 'macro' to account for label imbalance; it can result in an\n",
       "        F-score that is not between precision and recall. Weighted recall\n",
       "        is equal to accuracy.\n",
       "    ``'samples'``:\n",
       "        Calculate metrics for each instance, and find their average (only\n",
       "        meaningful for multilabel classification where this differs from\n",
       "        :func:`accuracy_score`).\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
       "    Sets the value to return when there is a zero division. If set to\n",
       "    \"warn\", this acts as 0, but warnings are also raised.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "recall : float (if average is not None) or array of float of shape              (n_unique_labels,)\n",
       "    Recall of the positive class in binary classification or weighted\n",
       "    average of the recall of each class for the multiclass task.\n",
       "\n",
       "See Also\n",
       "--------\n",
       "precision_recall_fscore_support : Compute precision, recall, F-measure and\n",
       "    support for each class.\n",
       "precision_score : Compute the ratio ``tp / (tp + fp)`` where ``tp`` is the\n",
       "    number of true positives and ``fp`` the number of false positives.\n",
       "balanced_accuracy_score : Compute balanced accuracy to deal with imbalanced\n",
       "    datasets.\n",
       "multilabel_confusion_matrix : Compute a confusion matrix for each class or\n",
       "    sample.\n",
       "PrecisionRecallDisplay.from_estimator : Plot precision-recall curve given\n",
       "    an estimator and some data.\n",
       "PrecisionRecallDisplay.from_predictions : Plot precision-recall curve given\n",
       "    binary class predictions.\n",
       "\n",
       "Notes\n",
       "-----\n",
       "When ``true positive + false negative == 0``, recall returns 0 and raises\n",
       "``UndefinedMetricWarning``. This behavior can be modified with\n",
       "``zero_division``.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import recall_score\n",
       ">>> y_true = [0, 1, 2, 0, 1, 2]\n",
       ">>> y_pred = [0, 2, 1, 0, 0, 1]\n",
       ">>> recall_score(y_true, y_pred, average='macro')\n",
       "0.33...\n",
       ">>> recall_score(y_true, y_pred, average='micro')\n",
       "0.33...\n",
       ">>> recall_score(y_true, y_pred, average='weighted')\n",
       "0.33...\n",
       ">>> recall_score(y_true, y_pred, average=None)\n",
       "array([1., 0., 0.])\n",
       ">>> y_true = [0, 0, 0, 0, 0, 0]\n",
       ">>> recall_score(y_true, y_pred, average=None)\n",
       "array([0.5, 0. , 0. ])\n",
       ">>> recall_score(y_true, y_pred, average=None, zero_division=1)\n",
       "array([0.5, 1. , 1. ])\n",
       ">>> # multilabel classification\n",
       ">>> y_true = [[0, 0, 0], [1, 1, 1], [0, 1, 1]]\n",
       ">>> y_pred = [[0, 0, 0], [1, 1, 1], [1, 1, 0]]\n",
       ">>> recall_score(y_true, y_pred, average=None)\n",
       "array([1. , 1. , 0.5])\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "recall_score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ec30ba5f-83b3-4916-b914-90b01f62e36b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for Model 1 is 80.33%\n"
     ]
    }
   ],
   "source": [
    "rec_mod_1 = recall_score(paws['actual'], paws['model1'], pos_label ='dog')\n",
    "print(f'The recall score for Model 1 is {rec_mod_1:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "306f09a2-6780-4ae5-9d81-b36957c200b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for Model 1 is 49.08%\n"
     ]
    }
   ],
   "source": [
    "rec_mod_2 = recall_score(paws['actual'], paws['model2'], pos_label ='dog')\n",
    "print(f'The recall score for Model 1 is {rec_mod_2:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f0401851-815b-496e-824f-7a8e52c23a83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for Model 1 is 50.86%\n"
     ]
    }
   ],
   "source": [
    "rec_mod_3 = recall_score(paws['actual'], paws['model3'], pos_label ='dog')\n",
    "print(f'The recall score for Model 1 is {rec_mod_3:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4a78c9ae-d932-4a4d-aee8-106bde69c4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recall score for Model 1 is 95.57%\n"
     ]
    }
   ],
   "source": [
    "rec_mod_4 = recall_score(paws['actual'], paws['model4'], pos_label ='dog')\n",
    "print(f'The recall score for Model 1 is {rec_mod_4:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c27198f-4133-4f1e-b252-c51a38e7e21d",
   "metadata": {},
   "source": [
    "#### 4. 4. sklearn.metrics.classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "94d7108d-258f-4bf7-a6ef-f19950af8892",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'warn'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Build a text report showing the main classification metrics.\n",
       "\n",
       "Read more in the :ref:`User Guide <classification_report>`.\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "y_true : 1d array-like, or label indicator array / sparse matrix\n",
       "    Ground truth (correct) target values.\n",
       "\n",
       "y_pred : 1d array-like, or label indicator array / sparse matrix\n",
       "    Estimated targets as returned by a classifier.\n",
       "\n",
       "labels : array-like of shape (n_labels,), default=None\n",
       "    Optional list of label indices to include in the report.\n",
       "\n",
       "target_names : list of str of shape (n_labels,), default=None\n",
       "    Optional display names matching the labels (same order).\n",
       "\n",
       "sample_weight : array-like of shape (n_samples,), default=None\n",
       "    Sample weights.\n",
       "\n",
       "digits : int, default=2\n",
       "    Number of digits for formatting output floating point values.\n",
       "    When ``output_dict`` is ``True``, this will be ignored and the\n",
       "    returned values will not be rounded.\n",
       "\n",
       "output_dict : bool, default=False\n",
       "    If True, return output as dict.\n",
       "\n",
       "    .. versionadded:: 0.20\n",
       "\n",
       "zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
       "    Sets the value to return when there is a zero division. If set to\n",
       "    \"warn\", this acts as 0, but warnings are also raised.\n",
       "\n",
       "Returns\n",
       "-------\n",
       "report : str or dict\n",
       "    Text summary of the precision, recall, F1 score for each class.\n",
       "    Dictionary returned if output_dict is True. Dictionary has the\n",
       "    following structure::\n",
       "\n",
       "        {'label 1': {'precision':0.5,\n",
       "                     'recall':1.0,\n",
       "                     'f1-score':0.67,\n",
       "                     'support':1},\n",
       "         'label 2': { ... },\n",
       "          ...\n",
       "        }\n",
       "\n",
       "    The reported averages include macro average (averaging the unweighted\n",
       "    mean per label), weighted average (averaging the support-weighted mean\n",
       "    per label), and sample average (only for multilabel classification).\n",
       "    Micro average (averaging the total true positives, false negatives and\n",
       "    false positives) is only shown for multi-label or multi-class\n",
       "    with a subset of classes, because it corresponds to accuracy\n",
       "    otherwise and would be the same for all metrics.\n",
       "    See also :func:`precision_recall_fscore_support` for more details\n",
       "    on averages.\n",
       "\n",
       "    Note that in binary classification, recall of the positive class\n",
       "    is also known as \"sensitivity\"; recall of the negative class is\n",
       "    \"specificity\".\n",
       "\n",
       "See Also\n",
       "--------\n",
       "precision_recall_fscore_support: Compute precision, recall, F-measure and\n",
       "    support for each class.\n",
       "confusion_matrix: Compute confusion matrix to evaluate the accuracy of a\n",
       "    classification.\n",
       "multilabel_confusion_matrix: Compute a confusion matrix for each class or sample.\n",
       "\n",
       "Examples\n",
       "--------\n",
       ">>> from sklearn.metrics import classification_report\n",
       ">>> y_true = [0, 1, 2, 2, 2]\n",
       ">>> y_pred = [0, 0, 2, 2, 1]\n",
       ">>> target_names = ['class 0', 'class 1', 'class 2']\n",
       ">>> print(classification_report(y_true, y_pred, target_names=target_names))\n",
       "              precision    recall  f1-score   support\n",
       "<BLANKLINE>\n",
       "     class 0       0.50      1.00      0.67         1\n",
       "     class 1       0.00      0.00      0.00         1\n",
       "     class 2       1.00      0.67      0.80         3\n",
       "<BLANKLINE>\n",
       "    accuracy                           0.60         5\n",
       "   macro avg       0.50      0.56      0.49         5\n",
       "weighted avg       0.70      0.60      0.61         5\n",
       "<BLANKLINE>\n",
       ">>> y_pred = [1, 1, 0]\n",
       ">>> y_true = [1, 1, 1]\n",
       ">>> print(classification_report(y_true, y_pred, labels=[1, 2, 3]))\n",
       "              precision    recall  f1-score   support\n",
       "<BLANKLINE>\n",
       "           1       1.00      0.67      0.80         3\n",
       "           2       0.00      0.00      0.00         0\n",
       "           3       0.00      0.00      0.00         0\n",
       "<BLANKLINE>\n",
       "   micro avg       1.00      0.67      0.80         3\n",
       "   macro avg       0.33      0.22      0.27         3\n",
       "weighted avg       1.00      0.67      0.80         3\n",
       "<BLANKLINE>\n",
       "\u001b[0;31mFile:\u001b[0m      /usr/local/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification_report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b4eebefa-ab1c-4794-99c1-b64638d99e55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Classification report for Model 1 is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cats       0.69      0.82      0.75      1746\n",
      "        Dogs       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_mod_1 = classification_report(paws['actual'],paws['model1'], target_names = ['Cats', 'Dogs'])\n",
    "print(f'The Classification report for Model 1 is:\\n{class_mod_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f7d10609-5887-43e6-8faa-11453d72a5ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Classification report for Model 2 is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cats       0.48      0.89      0.63      1746\n",
      "        Dogs       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_mod_2 = classification_report(paws['actual'],paws['model2'], target_names=['Cats', 'Dogs'])\n",
    "print(f'The Classification report for Model 2 is:\\n{class_mod_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "46b8035e-e588-42f8-838a-8baf330c961e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Classification report for Model 3 is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cats       0.36      0.51      0.42      1746\n",
      "        Dogs       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_mod_3 = classification_report(paws['actual'],paws['model3'], target_names=['Cats', 'Dogs'])\n",
    "print(f'The Classification report for Model 3 is:\\n{class_mod_3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "3fd00c4c-a27d-4674-986e-77eae2d45519",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Classification report for Model 4 is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Cats       0.81      0.35      0.48      1746\n",
      "        Dogs       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class_mod_4 = classification_report(paws['actual'],paws['model4'], target_names=['Cats', 'Dogs'])\n",
    "print(f'The Classification report for Model 4 is:\\n{class_mod_4}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
